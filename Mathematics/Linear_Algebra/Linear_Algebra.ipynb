{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Operators\n",
    "An operator $A: X \\rightarrow Y$ (read as operator A transforming a vector from vector space X to vector space Y) is said to be linear if, for every $\\mathbf{x}_1, \\mathbf{x}_2 \\in X$ and scalars $\\alpha_1, \\alpha_2 \\in R$ (R here denotes a ring and not real numbers),\n",
    "\n",
    "$$\n",
    "A(\\alpha_1\\mathbf{x}_1 + \\alpha_2\\mathbf{x}_2) = \\alpha_1 A(\\mathbf{x}_1) + \\alpha_2 A(\\mathbf{x}_2)\n",
    "$$\n",
    "\n",
    "Often the parantheses are dropped the operation in denoted as $A\\mathbf{x}$ - This shouldn't be confused with multiplying A with $\\mathbf{x}$. If $A$ is a matrix, then it is indeed a multiplication. But a matrix isn't the only linear operator. For example, a differential equation, $ \\dfrac{dx(t)}{dt} - ax(t) = b(t) $ can be thought of as a linear operator on $x(t)$ transforming it to $b(t)$, and can be written as:\n",
    "\n",
    "$$\n",
    "Ax(t) = b(t)\n",
    "$$\n",
    "\n",
    "where, $A$ is the \"differential operator\": $ A = \\dfrac{d}{dt} - a $. Linear operator theorey applies to all linear operators, and not just matrices.\n",
    "\n",
    "## Range and Nullity\n",
    "Range of an operator $A: X \\rightarrow Y$, $\\mathcal{R}(A)$ is the set of all vectors in $Y$ that are reachable by $A$, i.e., that can be results of the operation $A\\mathbf{x}$\n",
    "\n",
    "Nullity of an operator $A: X \\rightarrow Y$, $\\mathcal{N}(A)$ is the set of all vectors in $X$ that are transformed to $\\mathbf{0}$ in $Y$\n",
    "\n",
    "The nullity of any linear operator is a vector subspace of $X$. i.e., $\\mathcal{N}(A) \\subseteq X$. It is easy to see why: Suppose there exists a vector $\\mathbf{q}_1 \\in X$ that transforms to $\\mathbf{0}$ by $A$, then by definition of the linear operator, every vector $\\mathbf{x} = \\alpha \\mathbf{q}_1$ will also be transformed by $A$ to $\\mathbf{0}$. Similarly *if* there is another vector $\\mathbf{q}_2 \\in X$ that is linearly independent of $\\mathbf{q}_1$, that is in the nullity of $A$, then all linear combinations of $\\mathbf{q}_1$ and $\\mathbf{q}_2$ will also be transformed by $A$ to $\\mathbf{0}$. We can generalize this argument and say that, suppose we find $k$ linearly independent vectors that are in the nullity of $A$, then all linear combinations of those $k$ vectors would span a subspace. Hence the nullity of $A$ is also called, more popularly, as the **nullspace** of A. We can say that the nullspace has a Hamel basis: $\\{\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_k\\}$\n",
    "\n",
    "The dimension of the nullspace, $k$ could be $0$ - i.e., there may exists no vector in $X$, other than $\\mathbf{0}$, such that $A$ transforms it to $\\mathbf{0}$. In this case, $A$ is said to have a **trivial nullspace**, i.e., $\\mathcal{N}(A) = \\{\\mathbf{0}\\}$\n",
    "\n",
    "Suppose the nullspace is non-trivial, then if, $\\mathbf{x}_r \\in X$ is a solution to $A\\mathbf{x} = \\mathbf{b}$, then, $\\mathbf{x}_r + \\mathbf{x}_0$ is also a solution, for any $\\mathbf{x}_0 \\in \\mathcal{N}(A)$. In other words, if the nullspace is non-trivial, then $A\\mathbf{x} = \\mathbf{b}$ does not have a unique solution\n",
    "\n",
    "Since $X$ is a vector space, we know that it must have a Hamel basis. We also know that, since $\\mathcal{N}(A)$ is a subspace of $X$, we can find a Hamel basis of $X$ that contains, among other vectors, $\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_k $. Let us denote this Hamel basis as $ \\{\\mathbf{p}_1, \\mathbf{p}_2, \\ldots, \\mathbf{p}_r, \\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_k\\} $, where $ \\mathbf{p}_1, \\mathbf{p}_2, \\ldots, \\mathbf{p}_r $ are the other linearly indepedendent vectors in that Hamel basis. Given this, any vector in $X$ can be represented as:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = c_1\\mathbf{p}_1 + c_2\\mathbf{p}_2 + \\ldots + c_r\\mathbf{p}_r + c_{r+1}\\mathbf{q}_1 + c_{r+2}\\mathbf{q}_2 + \\ldots c_n\\mathbf{q}_k \n",
    "$$\n",
    "\n",
    "where $n$ is the dimension of $X$. Consequently, $A\\mathbf{x}$ can be represented as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A\\mathbf{x} &= c_1 A\\mathbf{p}_1 + c_2 A\\mathbf{p}_2 + \\ldots + c_r A\\mathbf{p}_r \\\\\n",
    "            &= c_1 \\mathbf{u}_1 + c_2 \\mathbf{u}_2 + \\ldots + c_1 \\mathbf{u}_r \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The $\\mathbf{q}_i$ terms go to $\\mathbf{0}$. $\\mathcal{R}(A)$, by definition, will have all possible linear combinations of $\\mathbf{u}_i = A\\mathbf{p}_i$. We can prove that the $\\mathbf{u}_i$ are linearly independent of one other. Suppose, let us assume that they are *not* linearly independent of one other. Then, we can find a set of scalars $\\{d_1, d_2, \\ldots, d_r\\}$, not all zeroes, such that,\n",
    "\n",
    "$$\n",
    "d_1 \\mathbf{u}_1 + d_2 \\mathbf{u}_2 + \\ldots + d_r \\mathbf{u}_r =0\n",
    "$$\n",
    "\n",
    "Substituting $\\mathbf{u}_i = A\\mathbf{p}_i$, we have,\n",
    "\n",
    "$$\n",
    "d_1 A\\mathbf{p}_1 + d_2 A\\mathbf{p}_2 + \\ldots + d_r A\\mathbf{p}_r = \\mathbf{0} \\\\\n",
    "A(d_1 \\mathbf{p}_1 + d_2 \\mathbf{p}_2 + \\ldots + d_r \\mathbf{p}_r) = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "Since the LHS of the last equation contains an operand entirely composed of $\\mathbf{p}_i$, none of which go to $\\mathbf{0}$ when operated upon by $A$, the only possible solution to that equation is if\n",
    "\n",
    "$$\n",
    "d_1 \\mathbf{p}_1 + d_2 \\mathbf{p}_2 + \\ldots + d_r \\mathbf{p}_r = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "Since the $\\mathbf{p}_i$ are linearly independent of one other, this is not possible, which mean that our assumption that $\\mathbf{u}_i$ are *not* linearly independent of one another has to be wrong! A beautiful consequence of this finding is that, $\\mathcal{R}(A)$ is not just a set of vectors, but a subspace of vectors. i.e., $\\mathcal{R}(A) \\subseteq Y$. And that its dimension is $r$, which is the number of vectors in the Hamel basis of $X$ after discounting the $\\mathbf{q}_is$. Since $\\mathcal{R}(A)$ is a subspace, it is often referred to as the **range space** of $A$.\n",
    "\n",
    "Since $Y$ is a vector space, we know that it must have a Hamel basis. We also know that, since $\\mathcal{R}(A)$ is a subspace of $Y$, we can find a Hamel basis of $Y$ that contains, among other vectors, $\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_r $. Let us denote this Hamel basis as $ \\{\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_r, \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_l\\} $, where $ \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_l $ are the other linearly indepedendent vectors in that Hamel basis. Given this, any vector in $Y$ can be represented as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = c_1\\mathbf{u}_1 + c_2\\mathbf{u}_2 + \\ldots + c_r\\mathbf{u}_r + c_{r+1}\\mathbf{v}_1 + c_{r+2}\\mathbf{v}_2 + \\ldots c_m\\mathbf{v}_l \n",
    "$$\n",
    "\n",
    "where $m$ is the dimension of $Y$.\n",
    "\n",
    "We see that the nullspace is a subspace of $X$ and that it is spanned by $\\mathbf{q}_is$. But what about the $\\mathbf{p}_is$? They must span a complementary subspace of $X$. Is there any point in analysing that subspace? Similarly, in the case of $Y$, we have a subspace complementary to the range space, that is spanned by the $\\mathbf{v}_is$. Is that subspace of any interest to us? It happens that they are interesting subspaces in their own right, but understanding their importance requires the introduction of the adjoint operator $A^*$.\n",
    "\n",
    "## Adjoint operator\n",
    "For a linear operator $A: X \\rightarrow Y$, there is an adjoint operator $A^*: Y \\rightarrow X$, such that, \n",
    "\n",
    "$$\n",
    "\\langle A\\mathbf{x}, \\mathbf{y} \\rangle = \\langle \\mathbf{x}, A^*\\mathbf{y} \\rangle\n",
    "$$\n",
    "\n",
    "We have seen the least squares solution in the case of underdetermined set of equations, $A\\mathbf{x} = \\mathbf{b}$ which is reproduced below for convenience:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = (A^H A)^{-1}A^H \\mathbf{b}\n",
    "$$\n",
    "\n",
    "But this is the case when,\n",
    "1. $A$ is a matrix\n",
    "2. The inner product is defined as: $\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\mathbf{y}^H \\mathbf{x} $\n",
    "\n",
    "Can the method of least squares be used when $A$ isn't a matrix? And when the inner product is defined differently? Yes. This is where the use of the adjoint operator becomes apparent. At the heart of the least squares is the fact that the error vector $\\mathbf{e} = \\mathbf{b} - A\\mathbf{x}$ is minimized when it is orthogonal to $A \\mathbf{x}$. Orthogonality itself is defined based on the inner product: Two vectors are said to be orthogonal if their inner product results in $0$. So, in the case of minimum error, we have:\n",
    "\n",
    "$$\n",
    "\\langle A\\mathbf{x}, \\mathbf{e} \\rangle = 0 = \\langle \\mathbf{x}, A^*\\mathbf{e} \\rangle\n",
    "$$\n",
    "\n",
    "Substituting, $\\mathbf{e} = \\mathbf{b} - A\\mathbf{x}$ in the RHS of the above equation, we have,\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{x}, A^*(\\mathbf{b} - A\\mathbf{x}) \\rangle = 0 \\\\\n",
    "\\langle \\mathbf{x}, A^*\\mathbf{b} \\rangle - \\langle \\mathbf{x}, A^*A\\mathbf{x} \\rangle = 0 \\\\\n",
    "\\langle \\mathbf{x}, A^*\\mathbf{b} \\rangle = \\langle \\mathbf{x}, A^*A\\mathbf{x} \\rangle\n",
    "$$\n",
    "\n",
    "The last equation is true only when,\n",
    "\n",
    "$$\n",
    "A^*A\\mathbf{x} = A^*\\mathbf{b} \\\\\n",
    "\\mathbf{x} = (A^*A)^{-1} A^*\\mathbf{b}\n",
    "$$\n",
    "\n",
    "The last equation is the least squares solution for any linear operator and any inner product. In the case of matrices, when the inner product is defined as $\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\mathbf{y}^H \\mathbf{x} $, we have,\n",
    "\n",
    "$$\n",
    "\\langle A\\mathbf{x}, \\mathbf{y} \\rangle = \\mathbf{y}^H A \\mathbf{x} \\\\\n",
    "\\text{And, } \\langle \\mathbf{x}, A^*\\mathbf{y} \\rangle = (A^*\\mathbf{y})^H \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Using the definition of adjoint, we can equate,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{y}^H A \\mathbf{x} &= (A^*\\mathbf{y})^H \\mathbf{x} \\\\\n",
    "                          & = \\mathbf{y}^H (A^*)^H \\mathbf{x}  \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Comparing both sides of the above equation, we get,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A &= (A^*)^H \\\\\n",
    "A^H &= ((A^*)^H)^H \\\\\n",
    "A^H &= A^*\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So for matrices, for the given inner product, $A^* = A^H$. Plugging this into the generalised least squares solution, we get our familiar least squares solution for matrices for the given inner product:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = (A^H A)^{-1}A^H \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Even in the case of matrices, if we were to change the definition of inner product, we can get a different least squares solution! For instance, if we define the inner product as:\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\mathbf{y}^H W \\mathbf{x}\n",
    "$$\n",
    "\n",
    "where $W$ is an invertible hermitian weighting matrix, we will find that, $A^* = W^{-1} A^H W$. So our least squares solution would change to:\n",
    "\n",
    "$$\n",
    "W^{-1} A^H W A \\mathbf{x} = W^{-1} A^H W b \\\\\n",
    "\\text{Since } W \\text{ is invertible, we can multiply either side of the equation, on the left with } W, \\text{ and get} \\\\\n",
    "A^H W A \\mathbf{x} = A^H W b \\\\\n",
    "\\mathbf{x} = (A^H W A)^{-1}A^H W\\mathbf{b}\n",
    "$$\n",
    "\n",
    "We will show later, how, in some cases, although, $A^{-1}$ and $(A^*)^{-1}$ may not exist, $(A^*A)^{-1}$ always exists. In fact, one can view the whole idea of least squares simply as that, because we cannot solve $A\\mathbf{x} = \\mathbf{b}$, simply as $\\mathbf{x} = A^{-1}\\mathbf{b}$, because $A$ isn't invertible, we can simply multiply both sides of $A\\mathbf{x} = \\mathbf{b}$ by $A^*$ and then we get our generalised least squares solution, because now $(A^*A)^{-1}$ would exist.\n",
    "\n",
    "The adjoint operator $A^*$ has the following properties:\n",
    "1. $ (A_1 + A_2)^* = A^*_1 + A^*_2 $\n",
    "2. $ (\\alpha A)^* = \\bar{\\alpha} A^* $, where $\\bar{\\alpha}$ is the complex conjugate of $\\alpha$ (check)\n",
    "3. $ (A_2 A_1)^* = A^*_1 A^*_2 $\n",
    "4. If $A$ has an inverse, then, $ (A^{-1})^* = (A^*)^{-1} $\n",
    "\n",
    "In a Hilbert space, we also have the property that $ A^{**} = A $\n",
    "\n",
    "## Range and nullspace of $A^*$\n",
    "**Theorem**\n",
    "The range of $A^*$ is perpendicular to the nullspace of A. ie., $ \\mathcal{R}(A^*) = [\\mathcal{N}(A)]^{\\perp} $\n",
    "\n",
    "*Proof*: \n",
    "Say, vector $\\mathbf{x}_n \\in \\mathcal{N}(A)$, then for *any* $\\mathbf{y} \\in Y$, \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\langle \\mathbf{x}_n, A^* \\mathbf{y} \\rangle &= \\langle A\\mathbf{x}_n, \\mathbf{y} \\rangle \\\\\n",
    "                                             &= \\langle \\mathbf{0}, \\mathbf{y} \\rangle \\\\\n",
    "                                             &= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This proves that $\\mathcal{N}(A) \\subset [\\mathcal{R}(A^*)]^\\perp$. i.e., the nullspace of $A$ lies within a space perpendicular to $\\mathcal{R}(A^*)$ - but it does not say if there is some vector $\\mathbf{x} \\in X$, where $\\mathbf{x} \\notin \\mathcal{N}(A)$ that is also in $[\\mathcal{R}(A^*)]^\\perp$. To prove that this is indeed the case, let us assume the contrary: That there is such an $\\mathbf{x}$. Then we have,\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{x}, A^* \\mathbf{y} \\rangle = 0 = \\langle A \\mathbf{x}, y \\rangle\n",
    "$$\n",
    "\n",
    "But since $\\mathbf{y}$ is *any* vector in $Y$, $\\langle A \\mathbf{x}, y \\rangle =0$ must be true for *all* the vectors in $Y$. That is only possible if $A\\mathbf{x} = 0$, which, by definition, means $\\mathbf{x}$ lies in $\\mathcal{N}(A)$. This goes against our assumption, and hence our assumption must be false. So $\\mathcal{N}(A)$ isn't just a subset of $[\\mathcal{R}(A^*)]^\\perp$, but, $\\mathcal{N}(A) = [\\mathcal{R}(A^*)]^\\perp$\n",
    "\n",
    "Using $A^{**} = A$, and using the above result, it can be easily be proven that we also have:  $\\mathcal{N}(A^*) = [\\mathcal{R}(A)]^\\perp$. With these two results, we have the following facts:\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(A) \\subset Y \\\\\n",
    "\\mathcal{N}(A) \\subset X \\\\\n",
    "\\mathcal{R}(A^*) \\subset X \\\\\n",
    "\\mathcal{N}(A^*) \\subset Y \\\\\n",
    "\\\\\n",
    "X = \\mathcal{R}(A^*) \\oplus \\mathcal{N}(A) \\\\\n",
    "Y = \\mathcal{R}(A) \\oplus \\mathcal{N}(A^*)\n",
    "$$\n",
    "\n",
    "The facts devolve from realising that, since we have already proven $\\mathcal{R}(A)$ and $\\mathcal{N}(A)$ to be subspaces, and that $\\mathcal{N}(A^*)$ $\\mathcal{R}(A^*)$ contain *all* the vectors orthogonal to them respectively, $\\mathcal{N}(A^*)$ $\\mathcal{R}(A^*)$ are themselves subspaces. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
