{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "Regression is the statistical analysis of relationships between a dependent (or \"outcome\") variable and one or more independent (or \"predictor\") variables. \n",
    "\n",
    "## Linear regression\n",
    "Linear regression is suitable if we suspect that a dependent variable can be expressed as a weighted sum of dependent variables as shown below.\n",
    "\n",
    "$$\n",
    "Y = \\alpha 1 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n + E\n",
    "$$\n",
    "\n",
    "where, E is the random variable representing the error. Note that it is called \"linear regression\" because $Y$ is linear in $\\alpha, \\beta_i$, not because it is linear in $X_i$. So $ Y = \\alpha + \\beta_1 X_1 + \\beta_2 X^2_1 + E $ is also considered a linear relationship. This is because one can think of a new random variable $ Z = X^2_1 $ and we will get a equation $ Y = \\alpha + \\beta_1 X_1 + \\beta_2 Z + E $. So linear regression covers representations of $Y$ as not only a line/plane/hyperplane, but also polynomial on the independent variables. \n",
    "\n",
    "Typically \\textbf{least-squares} method is used to solve for the coefficients, $\\alpha, \\beta_i$. Note that $1$ is used next to alpha to indicate that alpha is scaling a single-value random  variable. This may be absurd, but it is useful think about when we compute the least-squares solution.\n",
    "\n",
    "The first step in using least-squares method is to define the inner product (we will use induced norm). Inner product should be defined in a way where reducing $ \\langle E,E \\rangle $ makes sense: Probabilistic version of square error, i.e., $ \\langle E,E \\rangle = E[E,E] $ is a good choice, (i.e., $ \\langle X,Y \\rangle = E[XY] $). We will later see alternatives to this inner product.\n",
    "\n",
    "The simplest linear equation for $Y$ would be $Y = \\alpha + E$. It is silly because it is not dependent on any random variable $X_i$. But it is useful for understand more complex relationships. If we were to use the least-squares method to solve for $\\alpha$, we would get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\langle 1,1 \\rangle \\alpha &= \\langle Y,1 \\rangle \\\\\n",
    "\\alpha &= E[Y]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In other words, if one wants to get the smallest error prediction of $Y$ without considering its relationship with any underlying independent variable, then the best bet is to simply use $E[Y]$. For instance, if someone asks us what is the literacy rate of Madurai district, if we don't have a detailed relationship between literacy rate and independect factors such as average household income, number of schools etc. related to Madurai, the best bet is to just use the average literacy rate of Tamil Nadu. Surely Madurai would have a higher literacy rate because TN literacy rate as Madurai district is the cultural capital of TN. But using average TN literacy rate is a good idea, considering that we would use the same for answering literacy rates of all districts. That is, using the average literacy rate of TN would reduce the total (square) error in our predictions for all districts.\n",
    "\n",
    "Before we move on to serious regression, here are a couple of points about error E above:\n",
    "+ $\\mu_E = E[Y-\\mu_Y] = 0$\n",
    "+ $\\sigma^2_E = \\sigma^2_Y$\n",
    "\n",
    "These points will come handy when we evaluate the performance of serious regressions that use one or more dependent variables.\n",
    "\n",
    "### Simple linear regression (SLR)\n",
    "Going one step beyond $ Y = \\alpha + E $, we add just one dependent variable and try to model $Y$ as $ Y = \\alpha + \\beta X + E $. In a way, this is like asking ourselves how much better can we explain $Y$ - or reduce the error in $Y$ beyond just using $Y = E[Y] + E$. $ Y = \\alpha + \\beta X + E $ can be thought of as a line with $\\alpha$ as the y-intercept and $\\beta$ as the slope of the line. Using least-squares we have\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "\\langle 1,1 \\rangle & \\langle X,1 \\rangle \\\\ \n",
    "\\langle 1,X \\rangle & \\langle X,X \\rangle \n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\alpha \\\\\n",
    "\\beta\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\langle Y,1 \\rangle \\\\\n",
    "\\langle Y,X \\rangle \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When we solve for $\\alpha$ and $\\beta$, we get, \n",
    "\n",
    "$$\n",
    "\\alpha = \\dfrac{\\mu_Y E[X^2] - \\mu_X E[XY]}{E[X^2] - \\mu^2_X} \\\\\n",
    "\\beta = \\dfrac{E[XY]-\\mu_X\\mu_Y}{E[X^2] - \\mu^2_X}\n",
    "$$\n",
    "\n",
    "After a bit of massaging, it can be shown that,\n",
    "\n",
    "$$\n",
    "\\beta = \\dfrac{\\sigma^2_{XY}}{\\sigma^2_X} = \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} \\\\\n",
    "\\alpha = \\mu_Y - \\beta \\mu_X = \\mu_Y - \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} \\mu_X\n",
    "$$\n",
    "\n",
    "where, $\\sigma^2_{XY}$ is covariance of $X$ and $Y$. \n",
    "\n",
    "The resulting least-squares solution can be written as:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\mu_Y - \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} \\mu_X + \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} X\n",
    "$$\n",
    "\n",
    "where $\\hat{Y}$ is the least-squares estimate of $Y$. i.e., $ Y = \\hat{Y} + E $. After some rearranging of terms, one can write $Y$ itself as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(Y - \\mu_Y)\\ =\\ \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} (X - \\mu_X) + E\n",
    "\\label{eq:SLR_soln} \\tag{1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In fact, if, instead of finding the relationship between $Y$ and $X$, had we tried to find the (equivalent) relationship between the zero-mean variants, $Y' = Y-\\mu_Y$ and $X' = X-\\mu_X$, we would have solved for $Y' = \\alpha' + \\beta' X + E $ and we would have got $\\alpha' = 0$ and $\\beta = \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X}$, resulting in the exact same solution as above! In fact, one could further rearrage the equation and get \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left( \\dfrac{Y-\\mu_Y}{\\sigma_Y} \\right) = \\rho_{XY} \\left( \\dfrac{X-\\mu_X}{\\sigma_X} \\right) + E' \\\\\n",
    "Z_Y = Z_X + E'\n",
    "\\label{eq:SLR_standardized} \\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where, $Z_X$ and $Z_Y$ are standardized versions of $X$ and $Y$, and $E' = E/\\sigma_Y $ and $\\rho_{XY}$ is the correlation coefficient:\n",
    "\n",
    "$$\n",
    "\\rho_{XY} = \\dfrac{Cov(X,Y)}{\\sigma_X \\sigma_Y} = \\dfrac{\\sigma^2_{XY}}{\\sigma_X \\sigma_Y}\n",
    "$$\n",
    "\n",
    "Note that, if $X$ and $Y$ were gaussian random variables, the above equation represents the relationship between the \"standardized\" gaussian version of $X$ and $Y$, where both the dependent variable and independent variable have zero mean and unit standard deviation. Interestingly, this results in a revelation: That the correlation coefficient is simply the inner-product of standardised versions of two random variables!:\n",
    "\n",
    "$$\n",
    "\\rho_{XY} = \\langle Z_X,Z_Y \\rangle = E\\left[ \\left( \\dfrac{X-\\mu_X}{\\sigma_X} \\right) \\left( \\dfrac{Y-\\mu_Y}{\\sigma_Y} \\right) \\right]\n",
    "$$\n",
    "\n",
    "Here are some interesting points related to this revelation:\n",
    "+ When $\\rho_{XY}$ is 0, we say that $X$ and $Y$ are \"uncorrelated\". When $\\langle X,Y \\rangle = 0$, we say that $X$ and $Y$ are \"orthogonal\". Combining these two, one can say that, when two random variables are uncorrelated, their standardized versions will be orthogonal to each other.\n",
    "+ When two random variables are statistically independent, then they will also be uncorrelated. And if they are uncorrelated, their standardized versions are both uncorrelated and orthogonal. The importance of statistical independence will be apparent when we deal with Multiple Linear Regression (MLR)\n",
    "\n",
    "Coming back to our original least-squares solution form, let us first examine the characteristics of the error r.v. $E$ in equation $\\eqref{eq:SLR_soln}$.\n",
    "\n",
    "First, it is straight forward to show that, the error is zero mean:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_E\\ &=\\ E[(Y-\\mu_Y) - \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} (X-\\mu_X)] \\\\\n",
    "      &=\\ E[(Y-\\mu_Y)] - \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} E[(X-\\mu_X)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The stadard deviation of the error can be derived as below:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma^2_E\\ &=\\ E[E^2] - \\mu^2_E \\\\\n",
    "            &=\\ E[E^2] - 0 \\\\\n",
    "            &=\\ E[((Y-\\mu_Y) - \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} (X-\\mu_X) )^2] \\\\ \n",
    "            &=\\ \\sigma^2_Y + \\rho^2_{XY} \\dfrac{\\sigma^2_Y}{\\sigma^2_X} \\sigma^2_X - 2 \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} \\sigma^2_{XY}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which reduces to, \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma^2_E\\ = \\sigma^2_Y(1-\\rho^2_{XY})\n",
    "\\label{eq:error_var} \\tag{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$E$ is likely to be a Gaussian R.V. due to the implications of the central limit theorem. So what the above equation tells us is that, if, say, $\\rho_{XY} = 0.7$, then the error $E$'s variance will be approximate half of that of the dependent variable $Y$. One could phrase this as \"half of the variance of $Y$ is explained away by $X$\". Also, combined with the fact that $\\mu_E = 0$, we can now say things like, \"We are 68\\% confident that the prediction error is within $\\pm 0.7\\sigma_Y$\"\n",
    "           \n",
    "**A note about the choice of inner product**: Elementary to the derivation of the least-squares solution is the orthogonality property, where the error is orthogonal to every independent variable. i.e., $ \\langle X_i,E \\rangle = 0 $. Since we have defined the inner product as $ \\langle X, E \\rangle = E[XE] $ and since $\\mu_E = 0$, this means that $\\sigma^2_{\\hat{Y}E} = 0$. In probability theory parlance, they say that $\\hat{Y}$ and $E$ are \"uncorrelated\".\n",
    "\n",
    "### Real-world SLR\n",
    "We have so far skirted the issue that in LSR, we used population metrics such as $\\mu_X, \\sigma_Y$ etc., while in the real-world, no one knows these metrics. What we do is to come up with is sample metrics, which are unbiased estiamtes of the population metrics. So, instead of random variables, we have vectors containing respective observations. In other words, our least squares problem would look like:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \n",
    "= \n",
    "a \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} + \n",
    "b \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} + \n",
    "\\begin{bmatrix} e_0 \\\\ e_1 \\\\ \\vdots \\\\ e_n \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "The inner product in case of observation vectors is defined as: $ \\langle \\mathbf{X},\\mathbf{Y} \\rangle = \\sum_{i=1}^{n} x_iy_i$. This is missing a factor of $n$ compared to the unbiased estimate of $E[X,Y]$ which is the inner product we used while using population metrics. But it can be shown that this difference is of no consequence when calculating the coefficients and $a,b$ end up being the unbiased estimates of population variants $\\alpha, \\beta$. i.e., \n",
    "\n",
    "$$\n",
    "a = \\bar{y}- b \\bar{x} \\\\\n",
    "b = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\n",
    "$$\n",
    "\n",
    "where, $\\bar{x}$ is the unbiased estimate of $\\mu_X$. A sample equaivalent of $\\eqref{eq:error_var}$ also appears:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2\\ =\\ \\left(\\frac{1}{n} \\sum_{i=1}^{n}(y_i-\\bar{y})^2\\right) (1 - r^2)\n",
    "$$\n",
    "\n",
    "In $\\eqref{eq:error_var}$ we were computing the population error variance from the dependent variable's variance and the correlation coefficient of the dependent and independent variables. This was appropriate as it is assumed that the quantities in the right hand side of equation $\\eqref{eq:error_var}$ were considered to be known quantities and the error variance was the unknown quantities. But in the above sample variant of this equation, everything is an estimate. In fact, the error variance can be directly computed from the error observations. So the point of the equation becomes one of estimating the sample correlation coefficient! in other words, we rearrange the equation to make it more sensible:\n",
    "\n",
    "$$\n",
    "r^2\\ =\\ 1 - \\dfrac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i-\\bar{y})^2} \\\\\n",
    "r^2\\ =\\ 1 - \\dfrac{SSE}{SSTO}\n",
    "\\label{eq:sample_corr_coeff} \\tag{4}\n",
    "$$\n",
    "\n",
    "Notice that $\\frac{1}{n}$ factors in the numerator and denomination of the R.H.S. of the above equation got cancelled out. Here 'SSE' stands of Sum of Square (Errors) and 'SSTO' stands for Sum of Squares (Total). The reason why it is calle \"Total\" is because one can show that SSTO = SSE + SSR [(proof)](https://en.wikipedia.org/wiki/Explained_sum_of_squares), where 'SSR' is the Sum of Square (Regression), where,\n",
    "\n",
    "$$\n",
    "SSR = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y}_i)^2\n",
    "$$\n",
    "\n",
    "In other words, SSR is the part of SSTO that got \"explained away\" by the independent variable $X$ and SSE is the unexplained part remaining after the regression. One can rewrite equation $\\eqref{eq:sample_corr_coeff}$ as:\n",
    "\n",
    "$$\n",
    "r^2\\ =\\ \\dfrac{SSTO - SSE}{SSTO}\\ =\\ \\dfrac{SSR}{SSTO}\n",
    "$$\n",
    "\n",
    "So correlation coefficient $r^2$ is the percentage of SSTO that got \"explained away\" - or, since SSTO is related to variance of $Y$, it is the percentage of variance that got explained away. For this reason, the correlation coefficent $r^2$ is considered a \"**goodness-of-fit**\" measure of the regression result - i.e., how well $\\hat{y}$ is fitting $y$. Note that, the sign of the correlation coefficient gets lost in the above formula because of the squared. Alternatively one can use the formula below for the computation of $r$ directly so that the sign is not lost (and hence the directionality of the relationship between x and y is preserved), while one can square the $r$ and also gain insight into the goodness-of-fit.\n",
    "\n",
    "$$\n",
    "r=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "Statistical softwares typically display $r$ and mean squared error (MSE), where MSE is nothing but $\\frac{1}{n} SSE$. Sometimes a parameter called $s$ is displayed, which is the square root of MSE.\n",
    "\n",
    "There are more nuances w.r.to SLR. They are out of scope of this notebook. But one can explore them [here](https://online.stat.psu.edu/stat462/node/93/). Basically one can think of dependent variable $Y$ to have a distribution (instead of a single value) for every value of $X$ and the regression result as connecting the mean value of all these distributions. Without going further, we will just list the conditions that are requried for SLR to be appropriate, which can be remembered by the mnemonic <span style=\"color:red\">LINE</span>:\n",
    "+ The mean of $Y$ distribution must be a <span style=\"color:red\">L</span>inear function of $X$\n",
    "+ The errors $e_i$ are <span style=\"color:red\">I</span>ndependent\n",
    "+ The errors are <span style=\"color:red\">N</span>ormally distributed\n",
    "+ The errors at each value of $X$ have <span style=\"color:red\">E</span>qual variances\n",
    "\n",
    "### Multiple Regression (MLR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
