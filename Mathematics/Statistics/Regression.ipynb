{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "Regression is the statistical analysis of relationships between a dependent (or \"outcome\") variable and one or more independent (or \"predictor\") variables. \n",
    "\n",
    "## Linear regression\n",
    "Linear regression is suitable if we suspect that a dependent variable can be expressed as a weighted sum of dependent variables as shown below.\n",
    "\n",
    "$$\n",
    "Y = \\hat{Y} + E = \\alpha 1 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n + E\n",
    "$$\n",
    "\n",
    "where, E is the random variable representing the error and $\\hat{Y}$ is the regression result, or, the regression estimate of $Y$. Note that it is called \"linear regression\" because $Y$ is linear in $\\alpha, \\beta_i$, not because it is linear in $X_i$. So $ Y = \\alpha + \\beta_1 X_1 + \\beta_2 X^2_1 + E $ is also considered a linear relationship. This is because one can think of a new random variable $ Z = X^2_1 $ and we will get a equation $ Y = \\alpha + \\beta_1 X_1 + \\beta_2 Z + E $. So linear regression covers representations of $Y$ as not only a line/plane/hyperplane, but also polynomial on the independent variables. \n",
    "\n",
    "Typically \\textbf{least-squares} method is used to solve for the coefficients, $\\alpha, \\beta_i$. Note that $1$ is used next to alpha to indicate that alpha is scaling a single-value random  variable. This may be absurd, but it is useful think about when we compute the least-squares solution.\n",
    "\n",
    "The first step in using least-squares method is to define the inner product (we will use induced norm). Inner product should be defined in a way where reducing $ \\langle E,E \\rangle $ makes sense: Probabilistic version of square error, i.e., $ \\langle E,E \\rangle = E[E,E] $ is a good choice, (i.e., $ \\langle X,Y \\rangle = E[XY] $). We will later see alternatives to this inner product.\n",
    "\n",
    "The simplest linear equation for $Y$ would be $Y = \\alpha + E$. It is silly because it is not dependent on any random variable $X_i$. But it is useful for understand more complex relationships. If we were to use the least-squares method to solve for $\\alpha$, we would get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\langle 1,1 \\rangle \\alpha &= \\langle Y,1 \\rangle \\\\\n",
    "\\alpha &= E[Y]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In other words, if one wants to get the smallest error prediction of $Y$ without considering its relationship with any underlying independent variable, then the best bet is to simply use $E[Y]$. For instance, if someone asks us what is the literacy rate of Madurai district, if we don't have a detailed relationship between literacy rate and independect factors such as average household income, number of schools etc. related to Madurai, the best bet is to just use the average literacy rate of Tamil Nadu. Surely Madurai would have a higher literacy rate because TN literacy rate as Madurai district is the cultural capital of TN. But using average TN literacy rate is a good idea, considering that we would use the same for answering literacy rates of all districts. That is, using the average literacy rate of TN would reduce the total (square) error in our predictions for all districts.\n",
    "\n",
    "Before we move on to serious regression, here are a couple of points about error E above:\n",
    "+ $\\mu_E = E[Y-\\mu_Y] = 0$\n",
    "+ $\\sigma^2_E = \\sigma^2_Y$\n",
    "\n",
    "These points will come handy when we evaluate the performance of serious regressions that use one or more dependent variables.\n",
    "\n",
    "### Simple linear regression (SLR)\n",
    "Going one step beyond $ Y = \\alpha + E $, we add just one dependent variable and try to model $Y$ as $ Y = \\alpha + \\beta X + E $. In a way, this is like asking ourselves how much better can we explain $Y$ - or reduce the error in $Y$ beyond just using $Y = E[Y] + E$. $ Y = \\alpha + \\beta X + E $ can be thought of as a line with $\\alpha$ as the y-intercept and $\\beta$ as the slope of the line. Using least-squares we have\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "\\langle 1,1 \\rangle & \\langle X,1 \\rangle \\\\ \n",
    "\\langle 1,X \\rangle & \\langle X,X \\rangle \n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\alpha \\\\\n",
    "\\beta\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\langle Y,1 \\rangle \\\\\n",
    "\\langle Y,X \\rangle \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When we solve for $\\alpha$ and $\\beta$, we get, \n",
    "\n",
    "$$\n",
    "\\alpha = \\dfrac{\\mu_Y E[X^2] - \\mu_X E[XY]}{E[X^2] - \\mu^2_X} \\\\\n",
    "\\beta = \\dfrac{E[XY]-\\mu_X\\mu_Y}{E[X^2] - \\mu^2_X}\n",
    "$$\n",
    "\n",
    "After a bit of massaging, it can be shown that,\n",
    "\n",
    "$$\n",
    "\\beta = \\dfrac{\\sigma^2_{XY}}{\\sigma^2_X} = \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} \\\\\n",
    "\\alpha = \\mu_Y - \\beta \\mu_X = \\mu_Y - \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} \\mu_X\n",
    "$$\n",
    "\n",
    "where, $\\sigma^2_{XY}$ is covariance of $X$ and $Y$. \n",
    "\n",
    "The resulting least-squares solution can be written as:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\mu_Y - \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} \\mu_X + \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} X\n",
    "$$\n",
    "\n",
    "where $\\hat{Y}$ is the least-squares estimate of $Y$. i.e., $ Y = \\hat{Y} + E $. After some rearranging of terms, one can write $Y$ itself as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(Y - \\mu_Y)\\ =\\ \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} (X - \\mu_X) + E\n",
    "\\label{eq:SLR_soln} \\tag{1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In fact, if, instead of finding the relationship between $Y$ and $X$, had we tried to find the (equivalent) relationship between the zero-mean variants, $Y' = Y-\\mu_Y$ and $X' = X-\\mu_X$, we would have solved for $Y' = \\alpha' + \\beta' X + E $ and we would have got $\\alpha' = 0$ and $\\beta = \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X}$, resulting in the exact same solution as above! In fact, one could further rearrage the equation and get \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left( \\dfrac{Y-\\mu_Y}{\\sigma_Y} \\right) = \\rho_{XY} \\left( \\dfrac{X-\\mu_X}{\\sigma_X} \\right) + E' \\\\\n",
    "Z_Y = Z_X + E'\n",
    "\\label{eq:SLR_standardized} \\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where, $Z_X$ and $Z_Y$ are standardized versions of $X$ and $Y$, and $E' = E/\\sigma_Y $ and $\\rho_{XY}$ is the correlation coefficient:\n",
    "\n",
    "$$\n",
    "\\rho_{XY} = \\dfrac{Cov(X,Y)}{\\sigma_X \\sigma_Y} = \\dfrac{\\sigma^2_{XY}}{\\sigma_X \\sigma_Y}\n",
    "$$\n",
    "\n",
    "Note that, if $X$ and $Y$ were gaussian random variables, the above equation represents the relationship between the \"standardized\" gaussian version of $X$ and $Y$, where both the dependent variable and independent variable have zero mean and unit standard deviation. Interestingly, this results in a revelation: That the correlation coefficient is simply the inner-product of standardised versions of two random variables!:\n",
    "\n",
    "$$\n",
    "\\rho_{XY} = \\langle Z_X,Z_Y \\rangle = E\\left[ \\left( \\dfrac{X-\\mu_X}{\\sigma_X} \\right) \\left( \\dfrac{Y-\\mu_Y}{\\sigma_Y} \\right) \\right]\n",
    "$$\n",
    "\n",
    "Here are some interesting points related to this revelation:\n",
    "+ When $\\rho_{XY}$ is 0, we say that $X$ and $Y$ are \"uncorrelated\". When $\\langle X,Y \\rangle = 0$, we say that $X$ and $Y$ are \"orthogonal\". Combining these two, one can say that, when two random variables are uncorrelated, their standardized versions will be orthogonal to each other.\n",
    "+ When two random variables are statistically independent, then they will also be uncorrelated. And if they are uncorrelated, their standardized versions are both uncorrelated and orthogonal. The importance of statistical independence will be apparent when we deal with Multiple Linear Regression (MLR)\n",
    "\n",
    "Coming back to our original least-squares solution form, let us first examine the characteristics of the error r.v. $E$ in equation $\\eqref{eq:SLR_soln}$.\n",
    "\n",
    "First, it is straight forward to show that, the error is zero mean:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_E\\ &=\\ E[(Y-\\mu_Y) - \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} (X-\\mu_X)] \\\\\n",
    "      &=\\ E[(Y-\\mu_Y)] - \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} E[(X-\\mu_X)] = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In fact, the very reason why we model the linear regression as $ Y = \\alpha + \\beta X + E $, instead of $ Y = \\beta X + E $ is because, including a constant term $\\alpha$ scaling a \"random variable\" $1$ ensures $\\mu_E = 0$. This is because, elemental to the least squares method is the principle that the least error is orthogonal to every independent variable. This means,\n",
    "\n",
    "$$\n",
    "\\langle 1, E \\rangle = E[1E] = \\mu_E = 0 \n",
    "$$\n",
    "\n",
    "The reason why it is preferred that $\\mu_E=0$ is because $E$ is likely to be a Gaussian R.V. due to the implications of the central limit theorem. So, if $\\mu_E = 0$, we can directly look at the $\\sigma_E$ as the \"size\" of the error, because we know that there is a 68\\% prediction error is with $\\pm \\sigma_E$, 95% it is within $\\pm 2\\sigma_E$ and so on. \n",
    "\n",
    "\n",
    "The stadard deviation of the error can be derived as below:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma^2_E\\ &=\\ E[E^2] - \\mu^2_E \\\\\n",
    "            &=\\ E[E^2] - 0 \\\\\n",
    "            &=\\ E[((Y-\\mu_Y) - \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} (X-\\mu_X) )^2] \\\\ \n",
    "            &=\\ \\sigma^2_Y + \\rho^2_{XY} \\dfrac{\\sigma^2_Y}{\\sigma^2_X} \\sigma^2_X - 2 \\rho_{XY} \\dfrac{\\sigma_Y}{\\sigma_X} \\sigma^2_{XY}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which reduces to, \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma^2_E\\ = \\sigma^2_Y(1-\\rho^2_{XY})\n",
    "\\label{eq:error_var} \\tag{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "So what the above equation tells us is that, if, say, $\\rho_{XY} = 0.7$, then the error $E$'s variance will be approximate half of that of the dependent variable $Y$. One could phrase this as \"half of the variance of $Y$ is explained away by $X$\". Also, we can now say things like, \"We are 68\\% confident that the prediction error is within $\\pm 0.7\\sigma_Y$\"\n",
    "           \n",
    "**A note about the choice of inner product**: Elementary to the derivation of the least-squares solution is the orthogonality property, where the error is orthogonal to every independent variable. i.e., $ \\langle X_i,E \\rangle = 0 $. Since we have defined the inner product as $ \\langle X, E \\rangle = E[XE] $, this means that the covariance, $\\sigma^2_{\\hat{Y}E} = 0$. In probability theory parlance, they say that $\\hat{Y}$ and $E$ are \"uncorrelated\".\n",
    "\n",
    "A couple of interesting points about $\\hat{Y}$\n",
    "+ $E[\\hat{Y}] = E[Y]$. because, $\\mu_E = 0$. This means that the least squares estimate is an unbiased estimate.\n",
    "+ When the observed value of $X$ is $\\mu_X$, i.e., $x = \\mu_X$, the resulting value of $\\hat{Y}$ is $\\mu_Y$ because:\n",
    "$$\n",
    "\\begin{align}\n",
    "E[\\hat{Y}] &= \\alpha E[1] + \\beta E[X] \\\\\n",
    "\\mu_Y &= \\alpha + \\beta \\mu_X \n",
    "\\end{align}\n",
    "$$\n",
    "In other words, the coordinates $(\\mu_X, \\mu_Y)$ will always fall on the regression line! \n",
    "\n",
    "### Real-world SLR\n",
    "We have so far skirted the issue that in LSR, we used population metrics such as $\\mu_X, \\sigma_Y$ etc., while in the real-world, no one knows these metrics. What we do is to come up with is sample metrics, which are unbiased estiamtes of the population metrics. So, instead of random variables, we have vectors containing respective observations. In other words, our least squares problem would look like[<sup>[\\*]</sup>](#fn1).<a name = \"bffn1\"></a>:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \n",
    "= \n",
    "a \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} + \n",
    "b \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} + \n",
    "\\begin{bmatrix} e_0 \\\\ e_1 \\\\ \\vdots \\\\ e_n \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "The inner product in case of observation vectors is defined as: $ \\langle \\mathbf{X},\\mathbf{Y} \\rangle = \\sum_{i=1}^{n} x_iy_i$. This is missing a factor of $n$ compared to the unbiased estimate of $E[X,Y]$ which is the inner product we used while using population metrics. But it can be shown that this difference is of no consequence when calculating the coefficients and $a,b$ end up being the unbiased estimates of population variants $\\alpha, \\beta$. i.e., \n",
    "\n",
    "$$\n",
    "a = \\bar{y}- b \\bar{x} \\\\\n",
    "b = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\n",
    "$$\n",
    "\n",
    "where, $\\bar{x}$ is the unbiased estimate of $\\mu_X$. A sample equaivalent of $\\eqref{eq:error_var}$ also appears:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2\\ =\\ \\left(\\frac{1}{n} \\sum_{i=1}^{n}(y_i-\\bar{y})^2\\right) (1 - r^2)\n",
    "$$\n",
    "\n",
    "In $\\eqref{eq:error_var}$ we were computing the population error variance from the dependent variable's variance and the correlation coefficient of the dependent and independent variables. This was appropriate as it is assumed that the quantities in the right hand side of equation $\\eqref{eq:error_var}$ were considered to be known quantities and the error variance was the unknown quantities. But in the above sample variant of this equation, everything is an estimate. In fact, the error variance can be directly computed from the error observations. So the point of the equation becomes one of estimating the sample correlation coefficient! in other words, we rearrange the equation to make it more sensible:\n",
    "\n",
    "$$\n",
    "r^2\\ =\\ 1 - \\dfrac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i-\\bar{y})^2} \\\\\n",
    "r^2\\ =\\ 1 - \\dfrac{SSE}{SSTO}\n",
    "\\label{eq:sample_corr_coeff} \\tag{4}\n",
    "$$\n",
    "\n",
    "Notice that $\\frac{1}{n}$ factors in the numerator and denomination of the R.H.S. of the above equation got cancelled out. Here 'SSE' stands of Sum of Square (Errors) and 'SSTO' stands for Sum of Squares (Total). The reason why it is calle \"Total\" is because one can show that SSTO = SSE + SSR [(proof)](https://en.wikipedia.org/wiki/Explained_sum_of_squares), where 'SSR' is the Sum of Square (Regression), where,\n",
    "\n",
    "$$\n",
    "SSR = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y}_i)^2\n",
    "$$\n",
    "\n",
    "In other words, SSR is the part of SSTO that got \"explained away\" by the independent variable $X$ and SSE is the unexplained part remaining after the regression. One can rewrite equation $\\eqref{eq:sample_corr_coeff}$ as:\n",
    "\n",
    "$$\n",
    "r^2\\ =\\ \\dfrac{SSTO - SSE}{SSTO}\\ =\\ \\dfrac{SSR}{SSTO}\n",
    "$$\n",
    "\n",
    "So correlation coefficient $r^2$ is the percentage of SSTO that got \"explained away\" - or, since SSTO is related to variance of $Y$, it is the percentage of variance that got explained away. For this reason, the correlation coefficent $r^2$ is considered a \"**goodness-of-fit**\" measure of the regression result - i.e., how well $\\hat{y}$ is fitting $y$. Note that, the sign of the correlation coefficient gets lost in the above formula because of the squared. Alternatively one can use the formula below for the computation of $r$ directly so that the sign is not lost (and hence the directionality of the relationship between x and y is preserved), while one can square the $r$ and also gain insight into the goodness-of-fit.\n",
    "\n",
    "$$\n",
    "r=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "Statistical softwares typically display $r$ and mean squared error (MSE), where MSE is nothing but $\\frac{1}{n} SSE$. Sometimes a parameter called $s$ is displayed, which is the square root of MSE.\n",
    "\n",
    "There are more nuances w.r.to SLR. They are out of scope of this notebook. But one can explore them [here](https://online.stat.psu.edu/stat462/node/93/). Basically one can think of dependent variable $Y$ to have a distribution (instead of a single value) for every value of $X$ and the regression result as connecting the mean value of all these distributions. Without going further, we will just list the conditions that are requried for SLR to be appropriate, which can be remembered by the mnemonic <span style=\"color:red\">LINE</span>:\n",
    "+ The mean of $Y$ distribution must be a <span style=\"color:red\">L</span>inear function of $X$\n",
    "+ The errors $e_i$ are <span style=\"color:red\">I</span>ndependent\n",
    "+ The errors are <span style=\"color:red\">N</span>ormally distributed\n",
    "+ The errors at each value of $X$ have <span style=\"color:red\">E</span>qual variances\n",
    "\n",
    "The correlation coefficient $r^2$ is [often misinterpreted](https://online.stat.psu.edu/stat462/node/98/). It is good to keep in mind the following points.\n",
    "+ A small $r^2$ doesn't mean no trend exists between the dependent and independent variables. It only means no trend exists as the linear relationship we modeled  \n",
    "+ A large $r^2$ value doesn't indicate our model is the \"right\" model. Some other model could do still better. Also, one shouldn't assume that the model goes beyond the min, max values considered for the independent variable(s) while modeling. If a new value of an indendent variable is outside the boundary of the model, then the model may not be useful to make any predictions about the dependent variable\n",
    "+ Outliers can completely wreck the model and could lead to a bad, misleading fitting\n",
    "+ A large $r^2$ doesn't mean we can get a useful confidence interval for the dependent variable. The remaining variance may be so great that the model may not have achieved much\n",
    "+ Correlation doesn't meant causation: So a large $r^2$ doens't mean the independent variable(s) causes the dependent variable. Regression is often used in observational study where we look at existing data and arrive at some conclusions. But causation cannot be established through observational studies - one has to conduct an experiment (so one can create a controlled environment through careful design of the experiment)\n",
    "+ Ecological correlations — correlations that are based on rates or averages — tend to overstate the strength of an association. For instance, a person's education level and his/her salary may not be highly correlated. But if one looks at the average literacy level and the average per capita income of different states of India, we may get a much stronger association. \n",
    "+ A statistically significant $r^2$ doesn't imply that the slope parameters ($\\beta$s) are significantly different from 0\n",
    "\n",
    "### An expanded view of linear regression\n",
    "Let us take a step back and look at linear regression from space. The big picture is that, if one took into account every factor - i.e., independent variable - that can impact on the dependent variable, no matter how small that impact is, then the dependent variable isn't a random variable anymore: It will be a deterministic variable, whose value can be expressed with 100\\% accuracy given all the values of factors affecting it. In other words, we will have something like:\n",
    "\n",
    "$$\n",
    "Y = \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k \n",
    "$$ [<sup>[\\**]</sup>](#fn2).<a name = \"bffn2\"></a>\n",
    "\n",
    "Now, we may not know all the factors that affect $Y$, or, even if we know, we may not have the budget or precise instruments required to measure all factors. Basically it may be practically infeasible to measure all $X_i$s. So we want to get the best estimate $\\hat{Y}$ of $Y$ with whatever factors all available: For simplicity, let us say, that we consider only $X_1$, and replace everything else by a constant $\\alpha$:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\alpha + \\beta_1 X_1\n",
    "$$\n",
    "\n",
    "The best value of $\\alpha$ is the one that would minimize the error term: $Y-\\hat{Y}$. We can use extrema method from  differential calculus do this as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{d\\alpha} E[(Y - \\hat{Y})^2] &= 0 \\\\\n",
    "\\frac{d}{d\\alpha} E[(\\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k - \\alpha + \\beta_1 X_1)^2] &= 0  \\\\\n",
    "\\frac{d}{d\\alpha} E[(\\beta_2 X_2 + \\ldots + \\beta_k X_k - \\alpha)^2]  &= 0 \\\\\n",
    "\\frac{d}{d\\alpha} E[(\\beta_2 X_2 + \\ldots + \\beta_k X_k - \\alpha)^2]  &= 0 \\\\\n",
    "\\frac{d}{d\\alpha} E[(\\beta_2 X_2 + \\ldots + \\beta_k X_k)^2 + \\alpha)^2 - 2\\alpha(\\beta_2 X_2 + \\ldots + \\beta_k X_k)]  &= 0\n",
    "\\\\\n",
    "2\\alpha - 2E[(\\beta_2 X_2 + \\ldots + \\beta_k X_k)] &= 0 \\\\\n",
    "\\alpha &= E[(\\beta_2 X_2 + \\ldots + \\beta_k X_k)] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So, the constant term $\\alpha$ in a linear regression represents the mean of whatever terms were left out in the approximation. This ensures that, the mean of $Y$ doesn't change before and after the approximation because:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[Y]\\text{ (before approximation) } &= \\beta_1 E[X_1] + E[(\\beta_2 X_2 + \\ldots + \\beta_k X_k)] \\\\\n",
    "&= \\beta_1 E[X_1] + \\alpha = E[\\hat{Y}] \\\\\n",
    "&= E[Y] \\text{ (after approximation)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The last step is due to the fact that including $\\mu_E = 0$, which we have proved before. So given all this we can see the **big picture of linear regression**: The dependent variable $Y$ is a multidimensional function of all the $X_i$s. If one were to predict the value of $Y$, without any reference to $X_i$s, one has to contend with the entire probability distribution of $Y$ - For instance, if $Y$ is gaussian, we can say that there is a 68% chance that an observation of $Y$ will lie within $\\pm 1\\sigma_Y$ of $\\mu_Y$. But, say, we find out the relationship between $Y$ and $X_1$, then we can \"bucketise\" $Y$ values for every possible value of $X_1$. This is shown in the picture below.\n",
    "\n",
    "Now, if we measure $X_1$, that observation of $X_1$ would help us narrow down to just one bucket of possible $Y$ values - and we can make more accurate statements about $Y$ such as, \"Given $X_1 = x_1$, there is a 68\\% chance that an observation of $Y$ would lie within $\\pm 1\\sigma_B$ of $\\mu_B$\", where, \n",
    "\n",
    "$$\n",
    "\\mu_B = \\mu_Y | (X_1 = x_1) = \\beta_1 x_1 + E[(\\beta_2 X_2 + \\ldots + \\beta_k X_k)] \\\\\n",
    "\\begin{align}\n",
    "Var(B) = Var(Y | (X_1 = x_1)) &= Var(\\beta_1 x_1) + Var(\\beta_2 X_2 + \\ldots + \\beta_k X_k) \\\\\n",
    "                              &= 0 + Var(\\beta_2 X_2 + \\ldots + \\beta_k X_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\sigma_B$ hence would be more accurate that $\\sigma_Y$. This is why, we say that knowing the relationship between $Y$ and $X_1$, we have explained away part of the variance of $Y$. If we draw the regression line between $Y$ and $X_1$ and overlay all the observations of $Y$, we would see something like this (don't mind the annotations. This is a screengrab from PSU STAT 415 [page](https://online.stat.psu.edu/stat462/node/93/) about linear regression):\n",
    "\n",
    "<img src=\"LinearRegression.png\" width=1200/> \n",
    "\n",
    "For every possible value of $X_1$, we get a bunch of possible $Y$ values because of all possible values $\\beta_2 X_2 + \\ldots + \\beta_k X_k$ can take for a given value of $X_1$ - So we get a sub-distribution of $Y$ containing values of $Y$ belonging to that bucket $X_1 = x_1$. And the regression line would pass through the mean of that distribution, which is $ \\beta_1 x_1 + E[(\\beta_2 X_2 + \\ldots + \\beta_k X_k) $. So the regression line is the line that connects the means of all the sub-distributions of the buckets. \n",
    "\n",
    "---\n",
    "**Footnotes** \n",
    "\n",
    "<span id=\"fn1\"> *Although, for using least-squares method, we don't need vectors - just a proper definition of inner product is sufficient, one could think of a random variable as a vector that contains all possible observations of that random variable, with each observation appearing the same number of times as its relative frequency dictates - The length of that random variable can be the smallest set of observations required so that the least probable observation appears just one time. It can be more, but there won't be any point of that. For example, if we have a random variable $X_1$ that takes two values $0$ and $1$ with probabilities of $0.25$ and $0..75$ respectively, then a vector $[0,1,1,1]$ would sufficiently represent that random variable for vector algebra purposes - that is, if the variable is used alone. But, almost always, any meaningful calculation in linear algebra involves more than one vector. In that case, all vectors need to be long enough to accommodate all possible combinations of observations of all the random variables. i.e., if we have two random variables, $X_1$ as before, but also $X_2$ which also takes on two possible values $0$ and $1$, but with probabilities $0.5$ each, then our vectors need to look like:\n",
    "    \n",
    "$$\n",
    "\\mathbf{x_1} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\ \\text{and,}\\ \n",
    "\\mathbf{x_2} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is because we need to have $X_1 = 0$ for two possible values of $X_2$. This implies, $X_1$ must have at least two zeros. And because $1$ has to appear at least 3 times as much as $0$ for $X_1$, this means that $X_1$ needs to have at least six $1$s. Note the use of small, bold letters for the vectors - this is to indicate that they are traditional vectors, i.e., a collection of numbers.\n",
    "    \n",
    "If we talk about a linear equation such as $Y = \\beta_1 X_1 + \\beta_2 X_2 $, then it is enough that $X_1$ and $X_2$ are represented as above - Our observation based calculations will be representative of calculations involving population metrics. However, in reality, $X_1$ and $X_2$ may take so many possible values, or even take continuous values, that it may be impossible to replicate, say, a population inner product, $E[X_1 X_2]$ as an operation on two traditional vectors containing respresentative operations, like $\\mathbf{x_2}^T \\mathbf{x_1}$. So we settle down to observation vectors that can give, not a 100\\% accurate, but an unbiased, estimate of the random variables they represent.\n",
    "    \n",
    "<span id=\"fn2\"> **We could include a constant term $\\alpha$, but we will omit it because we will add a $\\alpha$ while approximating and we want to keep things simple and avoid confusions. Afterall, if $Y$ is indeed equal to $\\alpha + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k$, one could create a new $Y'$ which is equal to $Y - \\alpha$ and this $Y'$ will not have a constant term...we can continue our explanation with this $Y'$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
