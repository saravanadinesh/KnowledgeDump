{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sufficient statistic\n",
    "Any function of observations, i.e., $f(x_1, x_2, \\ldots, x_n)$ is technically a \"statistic\" (whether it is useful or not). The point of a statistic is to usually make some inferences about the distribution parameters from the statistic. Since a statistic compresses information contained in several observations into one value, a natural question is, once a statistic meant for making inferences about a distribution parameter is calculated, can we throw away the observations? - i.e., does the statistic contain all relevant information needed for us to make an inference about the distribution parameter of interest? The reason why we try to answer that question is not only that we don't need to store hundreds or thousands of observations (in the interest of conserving hard disk space, which, in this day and age, may not be a concern at all), but more importantly, it makes for convenient mathematics in many situations as we will see later.\n",
    "\n",
    "**Definition**\n",
    "Let $ X_1, X_2, \\ldots, X_n $ be a random sample from a probability distribution with unknown parameter $\\theta$. Then, the statistic:\n",
    "\n",
    "$$\n",
    "Y = u(X_1, X_2, ... , X_n)\n",
    "$$\n",
    "\n",
    "is said to be sufficient for $\\theta$ if the conditional distribution of $X_1, X_2, \\ldots, X_n$, given the statistic $Y$, doesn't depend on the parameter $\\theta$\n",
    "\n",
    "Note that this is not a theorem, but a definition. The intuition behind this choice of definition is as follows:\n",
    "\n",
    "1. The probability distribution of $ \\mathbf X = [X_1, X_2, \\ldots, X_n]^T $ is parameterised by $\\theta$\n",
    "2. If, now, knowing the value of $Y$ we could tell the probability of $\\mathbf{X}$ without knowing anything about $\\theta$, then it means, the impact that $\\theta$ had on the probability of $\\mathbf{X}$ is \"contained\" within $Y$. So the inverse must be true as well. \n",
    "\n",
    "## Factorisation theorem\n",
    "Let $ X_1, X_2, \\ldots, X_n $ denote random variables with joint probability density function or joint probability mass function $ f(x_1, x_2, \\ldots, x_n; \\theta) $, which depends on the parameter $\\theta$. Then, the statistic $ Y = u(X_1, X_2, \\ldots, X_n) $ is sufficient for $\\theta$ if and only if the p.d.f (or p.m.f.) can be factored into two components, that is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x_1, x_2, ... , x_n;\\theta) &= \\phi[ u(x_1, x_2, ... , x_n);\\theta ] h(x_1, x_2, ... , x_n) \\\\\n",
    "         f(\\mathbf{x};\\theta) &= \\phi[ y;\\theta ] h(\\mathbf{x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where, $\\phi$ is a function that depends on the data $ x_1, x_2, \\ldots, x_n $ only through the function $ u(x_1, x_2, \\ldots, x_n) $ and the function $h((x_1, x_2, \\ldots, x_n)$ does not depend on the parameter $\\theta$\n",
    "\n",
    "*Proof* (For the discrete case): Suppose we can factorize we can factorise the pmf as: $ P(\\mathbf{X};\\theta) = \\phi[u(\\mathbf{x});\\theta] h(\\mathbf{x})$, then we can find $P(Y = y)$ as shown below:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(Y=y) &= \\sum_{\\mathbf{x}: u(\\mathbf{x}) = y} P(X=x) \\\\\n",
    "       &= \\sum_{\\mathbf{x}: u(\\mathbf{x}) = y} \\phi[u(\\mathbf{x});\\theta] h(\\mathbf{x}) \\\\\n",
    "       &= \\phi [y;\\theta] \\sum_{\\mathbf{x}: u(\\mathbf{x}) = y} h(\\mathbf{x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, using this, we find that the conditional probability of $\\mathbf{x}$ conditioned on $Y=y$ is as given below:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\mathbf{X} = \\mathbf{x} | Y = y) &= \\dfrac{\\mathbf{X} = \\mathbf{x} , Y = y}{P(Y = y)} \\\\\n",
    "                          &= \\dfrac{\\phi[y;\\theta] h(\\mathbf{x})}{\\phi [y;\\theta] \\sum_{\\mathbf{x}: u(\\mathbf{x}) = y} h(\\mathbf{x})} \\\\\n",
    "                          &= \\dfrac{h(\\mathbf{x})}{\\sum_{\\mathbf{x}: u(\\mathbf{x}) = y} h(\\mathbf{x})}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since this conditional probability doesn't depend on $\\theta$, $y = u(\\mathbf{x})$ is a sufficient statistic.\n",
    "\n",
    "Conversely conditional distribution of $\\mathbf{X}$ given $Y=y$ is independent of $\\theta$, then, \n",
    "\n",
    "$$\n",
    "P(\\mathbf{X} = \\mathbf{x} | \\theta) = P(\\mathbf{X} = \\mathbf{x} | u(\\mathbf{x}) = y, \\theta) P(u(\\mathbf{x}) = y|\\theta) + P(\\mathbf{X} = \\mathbf{x} | u(\\mathbf{x}) \\neq y, \\theta) P(u(\\mathbf{x}) \\neq y|\\theta) \n",
    "$$\n",
    "\n",
    "The second term in the RHS obviously goes to zero. So we have,\n",
    "\n",
    "$$\n",
    "P(\\mathbf{X} = \\mathbf{x} | \\theta) = P(\\mathbf{X} = \\mathbf{x} | u(\\mathbf{x}) = y, \\theta) P(u(\\mathbf{x}) = y | \\theta)\n",
    "$$\n",
    "\n",
    "Since we started with the assumption that $Y$ is a sufficient statistic, given $Y=y$, the conditional probability of $\\mathbf{X}$ doesn't depend on $\\theta$: P(\\mathbf{X} = \\mathbf{x} | u(\\mathbf{x}) = y, \\theta) = P(\\mathbf{X} = \\mathbf{x} | u(\\mathbf{x}) = y $. Substituting this in the previus equation, we have:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{X} = \\mathbf{x} | \\theta) = P(\\mathbf{X} = \\mathbf{x}) P(Y = y | \\theta)\n",
    "$$\n",
    "\n",
    "which is in the desired form: $ P(\\mathbf{x};\\theta) = h(\\mathbf{x}) \\phi[y;\\theta] $\n",
    "\n",
    "# Exponential family\n",
    "## One parameter exponential family\n",
    "Exponential family of distributions have the following form:\n",
    "\n",
    "$$\n",
    "f_X(x|\\theta) = exp[\\ \\eta(\\theta)T(x) - A(\\theta)\\ ]\\ h(x)\n",
    "$$\n",
    "\n",
    "where, $T(x)$, $A(\\theta)$ and $h(x)$ are real valued functions, and $h(x) \\geq 0$. The term $exp(-A(\\theta))$ can be thought of as normalising the rest of the RHS above so that the area under the *pdf* will be 1. It is entirely determined by other functions.\n",
    "\n",
    "If it is a discrete RV, then the *pmf* looks the same. Sometimes an alternative notation is used: $ f_X(x|\\theta) = exp[\\ \\eta(\\theta)T(x) - A(\\theta) + B(x)\\ ] $. But we will use the first one in this notes. \n",
    "\n",
    "Bernoulli, Binomial, Poisson, Exponential, Normal, Gamme, Chi-squared are all examples of exponential family and their *pdf*s/*pmf*s can be written in the form stated above. All members of the exponential family have some common properties that make some mathematics related to them to become easy.\n",
    "\n",
    "A one parameter exponential family distribution where $\\eta$ is a one-to-one function of $\\theta$, i.e., then the distribution is said to be in **canonical exponential family**. In this case, $A(\\theta)$ can be written directly in terms of $\\eta$ as below:\n",
    "\n",
    "$$\n",
    "f(x|\\eta) = exp[\\ \\eta T(x) - A(\\eta)\\ ]\\ h(x)\n",
    "$$\n",
    "\n",
    "For canonical exponential family distributions, $\\eta$ is called the **natural parameter**. Again, $A(\\eta)$'s job is to normalise the distribution, i.e., \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{e}^{A(\\eta)} &= \\int_{R^d} \\mathcal{e}^{\\eta T(x)} h(x) < \\infty \\text{ (for continuous R.V.), or,} \\\\\n",
    "\\mathcal{e}^{A(\\eta)} &= \\sum_{x\\in\\mathcal{X}} \\mathcal{e}^{\\eta T(x)} h(x) < \\infty \\text{ (for discrete R.V.)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{R}^d$ is the range over which $X$ is defined. \n",
    "\n",
    "## Properties of canonical exponential family \n",
    "### Convexity\n",
    "It can be shown that the space $\\mathcal{N}$, on which which $\\eta$ is defined, is convex and that $A(\\eta)$ is convex function over that space [(more explanation)](https://www.stat.purdue.edu/~dasgupta/expfamily.pdf). i.e.,\n",
    "\n",
    "$$\n",
    "A(\\alpha\\eta_1 + (1-\\alpha)\\eta_2) \\leq  \\alpha A(\\eta_1) + (1-\\alpha) A(\\eta_2),\\ \\forall \\eta_1, \\eta_2 \\in \\mathcal{N} \n",
    "$$\n",
    "\n",
    "### Moments and MGF\n",
    "The function $A(\\eta)$ is infinitely differentiable at every $\\eta$. Furthermore, in the continuous case, $\\mathcal{e}^{A(\\eta)} = \\int_{R^d} \\mathcal{e}^{\\eta T(x)} h(x)$ can be  differentiated any number of times inside the integral, and in the discrete case, $ \\mathcal{e}^{A(\\eta)} = \\sum_{x\\in\\mathcal{X}} \\mathcal{e}^{\\eta T(x)} h(x) $ can be differentiated any number of times inside the summation. \n",
    "+ In the continuous case, for any $k \\geq 1$,\n",
    "$$\n",
    "\\dfrac{d^k}{d\\eta^k} \\mathcal{e}^{A(\\eta)} = \\int_{R^d} [T(x)]^k \\mathcal{e}^{\\eta T(x)} h(x)\n",
    "$$\n",
    "+ In the discrete case, for any $k \\geq 1$,\n",
    "$$\n",
    "\\dfrac{d^k}{d\\eta^k} \\mathcal{e}^{A(\\eta)} = \\sum_{x\\in\\mathcal{X}} [T(x)]^k \\mathcal{e}^{\\eta T(x)} h(x)\n",
    "$$\n",
    "\n",
    "Using this, we have the following results:\n",
    "+ $ E_\\eta[T(X)] = A'(\\eta) $\n",
    "+ $ Var_\\eta[T(X)] = A''(\\eta) $\n",
    "+ At any $t$ such that $\\eta+t \\in \\mathcal{N}$, the *mgf* of $T(X)$ exists and is: $ M_\\eta(t) = \\mathcal{e}^{A(\\eta+t) - A(\\eta)} $\n",
    "\n",
    "Another property corollary to these is that:\n",
    "+ $ E_\\eta[T(X)] $ is strictly increasing in $\\eta$. [Apparently]((https://www.stat.purdue.edu/~dasgupta/expfamily.pdf)) this means that the canonical exponential family can be reparameterised by using $E_\\eta[T(X)]$ instead of $\\eta$. I don't know what is the use of it. But it sounds cool.\n",
    "\n",
    "## Multi-dimensional one-parameter exponential family\n",
    "So far, we have assumed $X$ to be a scalar RV. But the exponential family can be extended to vectors just as well. One example of a multi-dimensional exponential family is when we make $n$ observations of an RV from an exponential family distribution and then look at their $n$-dimensional joint *pdf*. It would be:\n",
    "\n",
    "$$\n",
    "f_\\mathbf{X}(\\mathbf{x}|\\theta) = exp[\\ \\eta(\\theta)T(\\mathbf{x}) - A(\\theta)\\ ]\\ h(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Note that, although $\\mathbf{X}$ is a vector now, the function $T(\\mathbf{x})$ and $h(\\mathbf{x})$ are still scalars. All the theory about one-parameter scalar RV family that we discussed so far also apply to the multi-dimensional case.\n",
    "\n",
    "In the case of a multi-dimensional RV resulting from multiple observations of a single-dimensional exponential family RV, the function $T(\\mathbf{X})$ is a sufficient statistic, and is called the **natural sufficient statistic** of the parameter of that exponential family \n",
    "\n",
    "The following properties are true for any dimensional exponential family (we use multidimensional notation). For an RV \n",
    "+ $T = T(\\mathbf{X})$ is also an exponential family distribution\n",
    "+ Any RV $\\mathbf{Y} = A\\mathbf{X} + c$ also has a distribution in the exponential family\n",
    "+ If $\\mathcal{X}_0$ is a proper subset of $\\mathcal{X}_d$, then the joint conditional distribution of $\\mathbf{X} \\in \\mathcal{I}_0$ given $\\mathbf{X}^* \\in \\mathcal{I} - \\mathcal{I}_0$ is also an exponential family\n",
    "\n",
    "## Multi-parameter exponential family\n",
    "We have so far discussed single and multi-dimensional RV's but only in the context of one-parameter exponential families. For. ex., an n-dimensional gaussian RV with fixed variance (so mean is the only parameter). But we can have multi-parameter exponential families as well, and they have the same characteristics as the one-parameter ones. We state here the most general form of exponential family distributions where the parameter as well as the RV can be of any dimension (including 1-d). The *pdf* looks like:\n",
    "\n",
    "$$\n",
    "f_\\mathbf{X}(\\mathbf{x}; \\boldsymbol{\\theta}) = exp[\\ \\boldsymbol{\\eta}^T(\\boldsymbol{\\theta}) \\mathbf{T}(\\mathbf{x}) - A(\\boldsymbol{\\theta})\\ ]\\ h(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Note that, because $\\boldsymbol{\\theta}$ is a vector, $\\boldsymbol{\\eta}$ is a vector function, and consequently, $\\mathbf{T}$ is also a vector function. However, $A$ and $h$ are still scalar funcitons, although their arguments $\\boldsymbol{\\theta}$ and $\\mathbf{x}$ are both vectors.\n",
    "\n",
    "Canonical form (when $\\boldsymbol{\\eta}$ has a one-to-one relationship with $\\boldsymbol{\\theta}$) is as shown below:\n",
    "\n",
    "$$\n",
    "f_\\mathbf{X}(\\mathbf{x};\\boldsymbol{\\eta}) = exp[\\ \\boldsymbol{\\eta}^T \\mathbf{T}(\\mathbf{x}) - A(\\boldsymbol{\\eta})\\ ]\\ h(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "**Example**: The univariate Gaussian distribution parameterised by $\\boldsymbol{\\theta} = [\\mu\\ \\sigma]^T$ can be rewritten in canonical exponential family form as shown below:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_X(x;\\mu,\\sigma) &= \\dfrac{1}{\\sqrt{2\\pi}\\sigma} exp \\left( -\\dfrac{1}{2} \\dfrac{(x-\\mu)^2}{\\sigma^2} \\right) \\\\\n",
    "                  &= \\dfrac{1}{\\sqrt{2\\pi}} exp \\left( \\dfrac{\\mu}{\\sigma^2}x - \\dfrac{1}{2\\sigma^2}x^2 - \\dfrac{1}{2\\sigma^2}\\mu^2 - ln(\\sigma) \\right) \\\\\n",
    "                  &= \\dfrac{1}{\\sqrt{2\\pi}} exp \\left( [\\mu/\\sigma^2\\ 1/2\\sigma^2][x\\ x^2]^T - \\left( \\dfrac{\\mu^2}{2\\sigma^2}+ln(\\sigma)\\right) \\right) \\\\\n",
    "f(x;\\eta)         &= h(x) exp\\left( \\boldsymbol{\\eta}^T \\mathbf{T}(x) - A(\\boldsymbol{\\eta}) \\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\eta} &= \\begin{bmatrix} \\mu/\\sigma^2\\\\ 1/2\\sigma^2 \\end{bmatrix} \\\\\n",
    "\\mathbf{T}(x) &= \\begin{bmatrix} x\\\\ x^2 \\end{bmatrix} \\\\\n",
    "A(\\boldsymbol{\\eta}) &= \\dfrac{\\mu^2}{2\\sigma^2}+ln(\\sigma) = -\\dfrac{1}{2} \\dfrac{\\eta_1^2}{\\eta_2} - \\dfrac{1}{2} ln(-2\\eta_2)\\\\\n",
    "h(x) &= \\dfrac{1}{\\sqrt{2\\pi}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that, $h(x)$ doesn't actually have an $x$ term, which is OK, because one can think of $h(x)$ having the same value for all $x$s. \n",
    "\n",
    "We had seen, in the one-parameter canonical exponential family case, that, $E_\\eta[T(X)] = A'(\\eta)$ and $ Var_\\eta[T(X)] = A''(\\eta) $. In the multi-parameter case, these still hold with the differentiation replaced with gradient. i.e.,:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E_\\boldsymbol{\\eta}[\\mathbf{T}(X)] &= \\nabla_\\boldsymbol{\\eta} A(\\boldsymbol{\\eta}) \\\\\n",
    "Var_\\boldsymbol{\\eta}[\\mathbf{T}(X)] &= \\nabla^2_\\boldsymbol{\\eta} A(\\boldsymbol{\\eta})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In our example of mutlivariate gaussian, we hence get:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[ \\begin{bmatrix} X\\\\ X^2 \\end{bmatrix} ] &= \\begin{bmatrix} \\dfrac{d}{d\\eta_1} \\left( -\\dfrac{1}{2} \\dfrac{\\eta_1^2}{\\eta_2} - \\dfrac{1}{2} ln(-2\\eta_2) \\right)\\\\ \\dfrac{d}{d\\eta_2} \\left( -\\dfrac{1}{2} \\dfrac{\\eta_1^2}{\\eta_2} - \\dfrac{1}{2} ln(-2\\eta_2) \\right) \\end{bmatrix} \\\\\n",
    "\\text{and}\\\\\n",
    "Var[ \\begin{bmatrix} X\\\\ X^2 \\end{bmatrix} ] &= \\begin{bmatrix} \\dfrac{d^2}{d\\eta_1^2} \\left( -\\dfrac{1}{2} \\dfrac{\\eta_1^2}{\\eta_2} - \\dfrac{1}{2} ln(-2\\eta_2) \\right)\\\\ \\dfrac{d^2}{d\\eta_2^2} \\left( -\\dfrac{1}{2} \\dfrac{\\eta_1^2}{\\eta_2} - \\dfrac{1}{2} ln(-2\\eta_2) \\right) \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "One can work out the above equations. In the end, one would get:\n",
    "$$\n",
    "\\begin{align}\n",
    "E[ \\begin{bmatrix} X\\\\ X^2 \\end{bmatrix} ] &= \\begin{bmatrix} \\mu\\\\ \\mu^2+\\sigma^2 \\end{bmatrix} \\\\\n",
    "Var[ \\begin{bmatrix} X\\\\ X^2 \\end{bmatrix} ] &= \\begin{bmatrix} \\sigma^2\\\\ 4\\mu^2\\sigma^2+2\\sigma^4 \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which are as expected.\n",
    "\n",
    "## Maximum likelihood estimates\n",
    "When we make N observations from a canonical exponential family distribution, if they are independent observations, then their joint density funciton will be simply the multiplication of individual density functions. i.e., it will look like;\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\mathbf{x};\\boldsymbol{\\eta}) &= \\prod_{n=1}^N h(x_n) exp \\left( \\boldsymbol{\\eta}^T \\mathbf{T}(x_n) - A(\\boldsymbol{\\eta}) \\right) \\\\\n",
    "&= \\left(  \\prod_{n=1}^N h(x_n) \\right) exp \\left( \\boldsymbol{\\eta}^T \\sum_{n=1}^N \\mathbf{T}(x_n) - N A(\\boldsymbol{\\eta}) \\right) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When we try to find the MLE, we try to find the parameter that maximises the joint *pdf*, given the observations $(x_1, x_2, \\ldots, x_n)$. The joint *pdf* is renamed as the likelihood function with the role of the random variable and parameter flipped, like below:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta; \\mathbf{x}) = f(\\mathbf{x};\\theta)\n",
    "$$\n",
    "\n",
    "$\\theta$ here is just a placeholder of the parameter of interest and isn't the actual $\\theta$ in the exponential family contest.\n",
    "\n",
    "It is often the practice to try to maximize the log of the likelihood funciton instead of the likelihood function itself as it gives the same result, but is mathematically easier. In our case, the log likelihood function is as given below.\n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\eta}) = log \\left( \\prod_{n=1}^N h(x_n) \\right) + \\boldsymbol{\\eta}^T \\sum_{n=1}^N \\mathbf{T}(x_n) - N A(\\boldsymbol{\\eta})\n",
    "$$\n",
    "\n",
    "To get the MLE of $\\boldsymbol{\\eta}$, we take the gradient of $\\ell(\\boldsymbol{\\eta})$ and equate it to zero:\n",
    "\n",
    "$$\n",
    "\\nabla_\\boldsymbol{\\eta}(\\ell) = \\sum_{n=1}^N \\mathbf{T}(x_n) - N \\nabla_\\boldsymbol{\\eta}A(\\boldsymbol{\\eta}) = 0 \\\\\n",
    "\\nabla_\\boldsymbol{\\eta}A(\\boldsymbol{\\eta}^*) = \\dfrac{1}{N} \\sum_{n=1}^N \\mathbf{T}(x_n)\n",
    "$$\n",
    "\n",
    "where, $\\boldsymbol{\\eta}^*$ is our MLE for $\\boldsymbol{\\eta}$. Something interesting happens when we use the earlier result about canonical exponential family members, where, $E_\\boldsymbol{\\eta}[\\mathbf{T}(X)] = \\nabla_\\boldsymbol{\\eta} A(\\boldsymbol{\\eta})$. Substituting this in the above MLE solution, we get,\n",
    "\n",
    "$$\n",
    "E_\\boldsymbol{\\eta}[\\mathbf{T}(X)] = \\dfrac{1}{N} \\sum_{n=1}^N \\mathbf{T}(x_n)\n",
    "$$\n",
    "\n",
    "We always knew, intuitively, that the sample mean is a good estimate for mean of the original distribution (we are talking about the sample estimate and mean of $\\mathbf{T}$, not $X$) . Here we see that, mathematically it is accurate and is extendable to not just one parameter, but even to the multiparameter case (because here, $\\mathbf{T}$ is a vector). For example, in our Gaussian example case, we get,\n",
    "\n",
    "$$\n",
    "E_\\boldsymbol{\\eta}[\\begin{bmatrix} X\\\\ X^2 \\end{bmatrix}] = \\dfrac{1}{N} \\sum_{n=1}^N \\begin{bmatrix} x\\\\ x^2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "One can show that, for the exponential family the MLE of the sufficient statistic is unbiased.\n",
    "\n",
    "## References\n",
    "1. [Exponential family - Purdue](https://www.stat.purdue.edu/~dasgupta/expfamily.pdf) \n",
    "2. [Sufficiency - Missouri](https://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Sufficient.pdf)\n",
    "3. [Exponential family - Berkeley](https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
